<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="varun04reddy.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="varun04reddy.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-01T23:18:48+00:00</updated><id>varun04reddy.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Knowledge Editing</title><link href="varun04reddy.github.io/blog/2025/knowledge-editing/" rel="alternate" type="text/html" title="Knowledge Editing"/><published>2025-05-25T00:00:00+00:00</published><updated>2025-05-25T00:00:00+00:00</updated><id>varun04reddy.github.io/blog/2025/knowledge-editing</id><content type="html" xml:base="varun04reddy.github.io/blog/2025/knowledge-editing/"><![CDATA[<p>This post is a more casual commentary on my undergraduate thesis paper, <a href="https://arxiv.org/abs/2505.19383">CaseEdit</a>. While there are certainly things I wish I had implemented or articulated better, I’ll set those aside for now and focus on the core idea behind the project: knowledge editing in language models. Working on this topic gave me a foundational understanding of how knowledge is actually represented inside a model’s weights, and it helped me begin to make sense of the internal works of LLMs. Through that process, I found myself pulled deeper into the space of mech. interp. My hope is that this read offers a clearer sense of how knowledge is structured within LLMs: how it can be probed, located, and altered.</p> <hr/> <h3 id="background">Background</h3> <hr/> <p>Language models, during the pretraining stage, compress massive amounts of text and images into their parameters in order to learn the statistical patterns that support next-token prediction. Internet, being the largest and most diverse source of text and visual data, it naturally becomes the primary source of pretraining data. However, this only captures a snapshot of the world at a single point in time. While the model is being trained and deployed, the world continues to change, and so does the information landscape. As a result, the model’s knowledge is bottlenecked by the timestamp of that initial data snapshot. New facts emerge, and older ones may become outdated or incorrect. One way to address this is by fine-tuning the model on new data, but that approach is resource-intensive and risks forgetting of unrelated knowledge. Knowledge editing offers a more targeted alternative by directly modifying the model’s internal memory to update facts.</p> <p>In modern transformer architectures, factual associations are often embedded within the key and value structures of the model’s MLP blocks. These layers, combined with attention mechanisms, allows the model to retrieve and compose knowlege. During inference, attention heads compute a query that interacts with stored keys to retrieve values. These key-value pairs encode patterns of subject and subject-relation representations that map to factual objects. For example, when the model processes a prompt involving a specific subject, the internal activations form a query that retrieves the value associated with the key that matches that subject in context. For a more detailed explanation of keys, queries, and values in attention mechanisms, this <a href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms">Stack Exchange</a> post is great.</p> <p>Knowledge editing methods intervene in this process by identifying and modifying the parameters responsible for producing facts. These methods follow a locate and edit approach. First, they locate the regions of the network that encode a given fact. This involves techniques such as causal tracing or gradient-based attribution to isolate which layers are most influential in producing the original fact. Once identified, a small set of weights (most are found in the MLP layers) is modified to produce a new output for the same query (input). This allows the model’s behavior to be updated for specific facts without needing to retrain on large amounts of data or risk overwriting unrelated information.</p> <hr/> <h3 id="factual-knowledge-editing-methods">Factual Knowledge Editing Methods</h3> <hr/> <h4 id="memit">MEMIT</h4> <p><a href="https://arxiv.org/pdf/2210.07229">MEMIT</a> (Meng, 2023) is a method for performing localized factual edits in transformer models. The core inssight is that facts are often distributied accross various mid-layer MLPs.</p> <h5 id="identify-causal-layers">Identify Causal Layers</h5> <p>Through causal tracing, MEMIT locates the MLP layers most responsible for factual recall within the language model. In autoregressive models like GPT-J, this often corresponds to a middle block of layers (which is layers 3-8 in GPT-J). MEMIT uses a set of these layers, denoted as \(\mathcal{R}\), which are used as targets for editing.</p> <h5 id="distributed-weight-updates">Distributed Weight Updates</h5> <p>Instead of updating only one layer, MEMIT spreads the update across multiple layers within the selected \(\mathcal{R}\). For each fact we want to inject (represented as a subject-relation-object tuple \((s, r, o)\)), MEMIT determines a target hidden vector \(z_i\) which represents the desired hidden state at the final critical layer \(L\) (i.e., \(L = \max(\mathcal{R})\)) that would encode the new memory (essentially we are establishing a new target truth).</p> <figure style="text-align: center;"> <img src="/assets/img/blog/memit-fig.png" alt="MEMIT Diagram" width="700"/> <figcaption style="font-size: 0.95em; color: #555;">Figure 1: We see the the "critical path" where MLP layers process factual information about a subject. MEMIT directly edits these MLP modules to store new memories by adjusting the mapping from subject keys to memorized values. (Meng, 2023)</figcaption> </figure> <p>The goal is for the final hidden state \(h_i^L\) to become \(z_i\). This is done by iteratively modifying the MLP weights in each layer \(l \in \mathcal{R}\) in ascending order. For each layer \(l\), it computes the portion of the total required change \((z_i - h_i^L)\) that this specific layer should contribute to the residual stream. This portion, denoted as \(r_i^l\), is distributed as \(r_i^l = \frac{z_i - h_i^L}{L-l+1}\). This basically means that the deeper layers within \(\mathcal{R}\) (those with \(l\) closer to \(L\)) are responsible for a larger remaining portion of the change, while shallower layers contribute smaller fractions of the overall target residual needed.</p> <p>For each layer \(l\), MEMIT computes a direct parameter update \(\Delta^l\) for the MLP’s output weights (\(W_{out}^l\)). This update is calculated based on the layer’s input activations (keys, \(k_i^l\)) and the target output for that layer (\(m_i^l = W_{out}^l k_i^l + r_i^l\)). The update formula is given by:</p> \[\Delta^l = R^l (K^l)^T (C^l + K^l (K^l)^T)^{-1} \quad \text{}\] <p>Here, \(K^l\) is a matrix of input keys for all memories at layer \(l\), \(R^l\) is a matrix of the desired residual contributions \(r_i^l\), and \(C^l\) is a covariance value holding previously knowledge, which helps balance new vs. old associations. After each layer’s update, the model’s activations are re-collected to ensure that previous layers process the modified states correctly.</p> <hr/> <h4 id="alphaedit">AlphaEdit</h4> <p><a href="https://arxiv.org/pdf/2410.02355">AlphaEdit</a> (Fang, 2025) improves upon MEMIT by introducing null-space projection to reduce the “ripple effect” (knowledge disruptions) during editing. While MEMIT and similar methods attempt to preserve existing knowledge through a regularization term in their objective function, this often creates a trade-off between updating new information and retaining old, potentially leading to model forgetting or collapse (we saw this in the MEMIT update formula earlier).</p> <p>AlphaEdit, projects the parameter perturbation onto the null space of the preserved knowledge before applying it to the model’s weights. This guarantees that the preserved knowledge remains unaffected. This allows AlphaEdit to simplify its objective, focusing solely on minimizing the error of the new, to-be-updated knowledge without compromising existing information. This improvement in editing performance is achieved by adding just a single line of code for this projection (See Figure 2).</p> <figure style="text-align: center;"> <img src="/assets/img/blog/alphaedit.png" alt="AlphaEdit Diagram" width="700"/> <figcaption style="font-size: 0.95em; color: #555;">Figure 2: AlphaEdit improves upon MEMIT with one line of code. (Fang, 2025)</figcaption> </figure> <p>And the results are pretty impressive. This one line of code allows AlphaEdit to significantly reduce the ripple effect, enabling the number of edits to be scaled rapidly. As seen in Figure 3, conventional editing methods, after scaling the number of edits, tend to cause overlaps and convoluted changes, which reduces the quality and accuracy of edits. By solely focusing on updated knowledge via null-space projection, rather than trying to balance the optimization for preserved and updated knowledge, AlphaEdit is minimally affected by a scaled number of edits.</p> <figure style="text-align: center;"> <img src="/assets/img/blog/alphaedit-results.png" alt="AlphaEdit Results" width="700"/> <figcaption style="font-size: 0.95em; color: #555;">Figure 3: AlphaEdit reduces the ripple effect when scaling edits. (Fang, 2025)</figcaption> </figure> <hr/> <h4 id="memit-csk">MEMIT-CSK</h4> <p><a href="https://arxiv.org/pdf/2305.14956">MEMIT-CSK</a> (Gupta, 2023) is an extension of the original MEMIT editing algorithm, specifically designed to address the nuanced nature of <strong>commonsense knowledge</strong> in LLMs. Unlike normal MEMIT, which was primarily evaluated on factual knowledge with single correct answers, MEMIT-CSK addresses commonsense, characterized by uncertainty with multiple plausible answers. Applying the original MEMIT to commonsense judgments resulted in sub-par performance.</p> <p>Here is what MEMIT-CSK does to improve on MEMIT for commonsense:</p> <ul> <li> <p><strong>Varying Edit Tokens and Positions:</strong> MEMIT focuses on subject token editing], alternatively, MEMIT-CSK allows for editing at various token locations: subject, verb, and object. The core idea is that commonsense plausability depends on each part of the sentence.</p> </li> <li> <p><strong>Improved Layer Selection Strategy:</strong> MEMIT selects a five-layer window for editing whose last layer has the highest AIE (Average Indirect Effect) in the severed causal graph. MEMIT-SK improves this by also considering windows with the maximum <em>moving average</em> of AIE, leading to a more robust layer selection. Interestingly, moving AIE showed that commonsense is found in the earlier MLP layers rather than the middle layers (which is where factual knowledge is usally found).</p> </li> </ul> <p>For each fact to be edited, MEMIT-CSK follows a similar pattern to MEMIT by first determining a target hidden vector \(z_i\) at the final critical layer \(L\) to encode the new memory. This target is then distributed as a portion \(r_i^l = \frac{z_i - h_i^L}{L-l+1}\) across the MLP layers in the selected range \(\mathcal{R}\). The direct parameter update \(\Delta^l\) for the MLP’s output weights (\(W_{out}^l\)) is then computed based on the layer’s input activations (keys, \(k_i^l\)) and the target output for that layer (\(m_i^l = W_{out}^l k_i^l + r_i^l\)).</p> <hr/> <h3 id="caseedit">CaseEdit</h3> <p>So how does do these various knowledge edting methods tie together for my thesis? CaseEdit serves as both a dataset creation and an evaluation pipeline specifically designed for commonsense knowledge editing in small-parameter language models. This application is targeted toward personalized, locally hosted home devices, such as edge compute devices that cannot host larger-parameter models. CaseEdit’s objective is to test the plausibility of knowledge editing for household LLMs. The pipeline achieves this by generating plausible commonsense household knowledge edits using a larger-parameter model and producing unique evaluation questions that assess the usefulness of these edits. We then apply flagship knowledge editing techniques to a CaseEdit dataset to compute valuable metrics.</p> <h4 id="commonsense-edits-generation-caseedit">Commonsense Edits Generation (CaseEdit)</h4> <p>Our commonsense edits are generated through a multi-stage inference process utilizing a higher-parameter language model (GPT-4o-mini).</p> <ul> <li> <p><strong>Step 1: Generate Atypical Location.</strong> First, for each selected household object (subject), an independent inference loop is used to propose an unusual, yet plausible, everyday household location. For example, while a butter knife is typically found in the kitchen, our language model might suggest an unusual location like a garage.</p> </li> <li> <p><strong>Step 2: Generate New Ground Truth.</strong> In a subsequent inference loop, we provide the household item and the generated unusual location. The model is then prompted to generate an atypical use or property for that item, conditioned on a randomly assigned “Plausibility Bucket” (See <a href="https://arxiv.org/pdf/2010.05953">ATOMIC 2020</a> for more about this). This establishes a new target truth for the edit (See Table 1).</p> </li> </ul> <figure style="text-align: center;"> <img src="/assets/img/blog/caseedit-table.png" alt="Caseedit Table1" width="700"/> <figcaption style="font-size: 0.95em; color: #555;">Examples of CaseEdit knowledge editing chain creation pipeline. (Reddy, 2025)</figcaption> </figure> <p>For the <strong>Evaluation Question Generation</strong>, additional inference loop is employed to create four types of multiple-choice questions for each edit. These questions are tailored to evaluate the knowledge edits across key metrics (See Table 2):</p> <ul> <li><strong>Reliability:</strong> Directly assesses if the edit successfully modifies the model’s output for the specific input.</li> <li><strong>Generalization:</strong> Evaluates if the model correctly applies the edited knowledge to semantically similar inputs or paraphrased versions.</li> <li><strong>Locality:</strong> Determines if the edit unintentionally alters predictions on unrelated inputs, assessing for negative “ripple effects”.</li> <li><strong>Portability:</strong> Measures if the newly acquired knowledge can be correctly applied in more complex, multi-hop reasoning scenarios or downstream tasks.</li> </ul> <figure style="text-align: center;"> <img src="/assets/img/blog/caseedit-table2.png" alt="CaseEdit Table 2" width="700"/> <figcaption style="font-size: 0.95em; color: #555;">Examples of CaseEdit evaluation questions. Tokens activating the edited layer are highlighted in blue, while potentially entangled tokens that should remain unchanged are highlighted in red.(Reddy, 2025)</figcaption> </figure> <p>We posit that larger parameter models inherently possess a more robust understanding of commonsense. This distinction forms the bedrock of our approach for two key reasons:</p> <ol> <li> <p><strong>Enabling Personalized Small-Parameter Assistants:</strong> Smaller parameter models are ideal for edge computing environments due to their lightweight architecture, real-time adaptability, and energy efficiency. These models inherently face challenges in adapting to highly personalized or context-specific commonsense requirements. We therefore advocate for knowledge editing as an effective method to imbue these smaller models with the necessary, context-driven commonsense, allowing them to function as more effective and personalized assistants.</p> </li> <li> <p><strong>Leveraging Large Models for Ground Truth Generation:</strong> Our confidence in using a larger parameter model (specifically, GPT-4o-mini) for generating new commonsense ground truths stems from their demonstrated superior capabilities in reasoning and adapting to nuanced contexts. This allows us to curate high-quality, atypical household-specific knowledge, which then serves as the target for editing in smaller models.</p> </li> </ol> <p>To illustrate the difference in inherent commonsense capabilities, beyond the scope of direct editing performance, we conducted an evaluation using a generated CaseEdit dataset. This involved assessing a small-parameter Llama-3.1-8B model against a larger-parameter Llama-3.1-70B model. In this setup, we used an altered version of our multiple-choice questions, removing the original ground truth and replacing it with a throwaway option. For each answer the models choose, we prompt them to provide an explanation. The alignment of these explanations with the chosen answer is assessed, ideally by a human evaluator, but in our case a larger parameter: Llama 3.1 405B.</p> <table style="width:100%; border-collapse:collapse;"> <tr> <th style="padding: 8px;">Model</th> <th style="padding: 8px;">Reliability</th> <th style="padding: 8px;">Generalization</th> <th style="padding: 8px;">Locality</th> <th style="padding: 8px;">Portability</th> </tr> <tr> <td style="padding: 8px;">Llama-3.1-8B</td> <td style="padding: 8px;">65%</td> <td style="padding: 8px;">61%</td> <td style="padding: 8px;">54%</td> <td style="padding: 8px;">39%</td> </tr> <tr> <td style="padding: 8px;">Llama-3.1-70B</td> <td style="padding: 8px;">84%</td> <td style="padding: 8px;">81%</td> <td style="padding: 8px;">82%</td> <td style="padding: 8px;">79%</td> </tr> </table> <hr/> <p>Our experiments, particularly with AlphaEdit, yielded impressive results. AlphaEdit consistently outperformed other KE methods on CaseEdit across all evaluated metrics, demonstrating its strong performance even in personalized commonsense KE. Additonally, as we scaled the number of edits, AlphaEdit showed minimal ripple effects.</p> <p>We also analyzed the model’s confidence and uncertainty as edits scaled. This was done by examining the softmax probability distribution over the five multiple-choice answers, derived from the model’s output logits. To quantify this uncertainty, we employed <strong>Shannon entropy</strong> \(H(p)\) of the probability distribution:</p> \[H(p) = -\sum_{i \in \{A,B,C,D,E\}} p_i \log_2(p_i) \quad\] <p>As seen in Figure 4:</p> <figure style="text-align: center;"> <img src="/assets/img/blog/caseedit-logit.png" alt="CaseEdit Table 2" width="700"/> <figcaption style="font-size: 0.95em; color: #555;">Next-token probabilities and entropy during MCQ evaluation using AlphaEdit on CaseEdit.</figcaption> </figure> <p>When we see the model initially placing a high probability on the original, pre-edit truth, it tells us that this piece of information is very strongly and consistently encoded within the LLM’s vast set of parameters. It’s a clear signal that the model’s internal knowledge representation firmly aligns with that fact.</p> <p>Now, after we’ve applied a single edit, and the probability mass has largely shifted to our new ground truth, this signifies a successful intervention. The editing method has effectively rerouted the model’s internal associations, so that when queried, it now confidently points to the updated information. However, the presence of residual probability on the old truth or other options indicates that the edit isn’t always a complete overwrite; it might introduce a slight degree of ambiguity or retain faint, underlying connections to the prior information.</p> <p>The real technical challenge, and what Figure 4 really highlights, comes with sequential editing. The gradual decrease in probability for a newly edited fact, combined with an increase in overall uncertainty, shows us the “ripple effect” in action at a fundamental level. It means that as we modify more and more parts of the model’s knowledge, even seemingly unrelated edits can subtly interfere with previously updated facts, causing the model’s internal confidence in those earlier edits to dilute.</p> <p>This is where AlphaEdit shines: by projecting parameter perturbations into the null space of preserved knowledge, it minimizes cumulative interference. It means the model’s internal representation of each individual edited fact is more isolated and therefore less prone to “forgetting” or “overwriting” when subsequent knowledge updates occur.</p> <hr/> <h3 id="conclusion">Conclusion</h3> <p>In this post, we walked through the landscape of knowledge editing in language models. Along the way, we introduced CaseEdit—a dataset and evaluation pipeline built for probing commonsense edits in small-parameter LLMs. We also explored how techniques like MEMIT and AlphaEdit interact with a model’s internal memory, each bringing its own approach to the difficult task of rewriting what a model “knows.”</p> <p>That said, there is still a long road ahead. Many directions remain untapped, partly due to the usual suspects—limited compute and time—but also because this space is still very much under active construction.</p> <p><strong>RAG vs. Weight-Based Editing</strong><br/> An open question that keeps coming up is how weight-based knowledge editing compares to retrieval-based approaches like RAG. Both promise to update a model’s behavior, but with very different philosophies. One rewires the model’s internals directly, while the other fetches knowledge from an external source at inference time. A compelling next step would be to evaluate both paradigms under CaseEdit’s contextual edit tasks, especially those that involve subtle multi-hop commonsense. My guess is that each will have blind spots that the other can help cover.</p> <p><strong>Blending Mechanisms for a Hybrid Approach</strong><br/> I am also curious about what happens when we mix the strengths of MEMIT-CSK and AlphaEdit. MEMIT-CSK stands out for its flexible editing across token types and smarter layer selection. AlphaEdit brings a crisp linear algebra perspective with its null-space projection strategy. It seems natural to ask whether AlphaEdit’s minimal injection style could be merged with MEMIT-CSK’s more distributed edit process. A hybrid method might capture the best of both worlds—structured propagation with tight generalization control.</p> <p><strong>Tracing Internal Model Circuits:</strong><br/> Lastly, there is the interpretability angle. We still do not fully understand what happens inside the model after an edit is made. What changes, exactly? Which pathways reconfigure themselves? Future work could trace the circuits involved before and after an edit, mapping how the model’s “belief network” morphs to accommodate a new fact. This might be one of the most promising ways to turn black-box interventions into something more surgical and predictable.</p> <p>Working on knowledge editing gave me a much deeper understanding of how large language models store and access information. I believe anyone interested in the internals of these systems can benefit from exploring this area, even briefly. By learning how factual edits propagate through a model’s layers, I began to form a more concrete intuition about where information resides—how it is encoded across MLP layers, how it interacts with attention, and how specific facts emerge from distributed representations. It is not interpretable in the way humans might prefer, but it is consistent and surprisingly structured once you know where to look.</p> <p>That said, I have come to believe that knowledge editing is unlikely to be the long-term solution for keeping models up to date. There are two main reasons. First, efficiency. Even while working with a relatively small model, I ran into bottlenecks related to compute and time. Making hundreds of targeted edits, validating each one, and managing interference is far from lightweight. Second, effectiveness. While editing methods like MEMIT and AlphaEdit work well in many cases, they are not universally reliable. They can struggle with generalization, or with preserving fluency in edge cases.</p> <p>This is why I suspect that state-of-the-art systems will continue leaning toward tool calling with search, rather than purely parameter-based updates. Tooling with search provides fresher knowledge at inference time, with minimal risk to model coherence or memory. Still, knowledge editing has value. If nothing else, it offers a lens into the the black boxiness of LLMs which can help inform future work on model interpretability and internal representation design.</p> <h4 id="references">References</h4> <hr/> <ul> <li> <p>Meng, K., Sen Sharma, A., Andonian, A. J., Belinkov, Y., &amp; Bau, D. (2023). <em>Mass-editing memory in a transformer</em>. ICLR 2023.</p> </li> <li> <p>Fang, J., Jiang, H., Wang, K., Ma, Y., Shi, J., Wang, X., He, X., &amp; Chua, T.-S. (2025). <em>AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models</em>. ICLR 2025.</p> </li> <li> <p>Reddy, V., Kuo, Y. (2025). <em>CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models</em>. UVA SEAS Library.</p> </li> <li> <p>Gupta, A., Mondal, D., Sheshadri, A. K., Zhao, W., Li, X. L., Wiegreffe, S., &amp; Tandon, N. (2023). <em>Editing Common Sense in Transformers</em>. EMNLP 2023.</p> </li> <li> <p>Jena D. Hwang and Chandra Bhagavatula and Ronan {Le Bras} and Jeff Da and Keisuke Sakaguchi and Antoine Bosselut and Yejin Choi. (2021). <em>(COMET)ATOMIC2020:On Symbolic and Neural Commonsense Knowledge Graphs</em>. AAAI 2021.</p> </li> </ul>]]></content><author><name></name></author><category term="technical"/><summary type="html"><![CDATA[This post is a more casual commentary on my undergraduate thesis paper, CaseEdit. While there are certainly things I wish I had implemented or articulated better, I’ll set those aside for now and focus on the core idea behind the project: knowledge editing in language models. Working on this topic gave me a foundational understanding of how knowledge is actually represented inside a model’s weights, and it helped me begin to make sense of the internal works of LLMs. Through that process, I found myself pulled deeper into the space of mech. interp. My hope is that this read offers a clearer sense of how knowledge is structured within LLMs: how it can be probed, located, and altered.]]></summary></entry><entry><title type="html">College Reflections</title><link href="varun04reddy.github.io/blog/2025/reflect-undergrad/" rel="alternate" type="text/html" title="College Reflections"/><published>2025-05-20T00:00:00+00:00</published><updated>2025-05-20T00:00:00+00:00</updated><id>varun04reddy.github.io/blog/2025/reflect-undergrad</id><content type="html" xml:base="varun04reddy.github.io/blog/2025/reflect-undergrad/"><![CDATA[<p><em>Reflections after graduating from the University of Virginia</em></p> <p>I recently graduated from UVA, and over the past few weeks I have felt a quiet obligation to reflect on the lessons that emerged. Some of these lessons have been repeated countless times, while others might be more specific to my own experience. I believe there is value in both.</p> <p>It is often said that college is not about education as much as it is about learning how to live. I think that is accurate. In many ways, these four years serve as a transitional state, an intentionally imperfect rehearsal for adulthood.</p> <hr/> <h2 id="1-time-and-scheduling">1. Time and scheduling.</h2> <p>Unlike high school, where your day is structured down to the minute, college hands you a blank slate. No one checks whether you attend class, eat lunch, or sleep before 4 a.m. The freedom is immediate—and initially, disorienting. My first semester became a series of experiments in self-observation: tracking energy levels, gauging my mental and physical health, and gradually learning when I function best. Once I figured out a system that worked for me, life became more predictable. Not easier, but definitely more manageable. One of the most helpful realizations was understanding whether I’m a morning person or a night person—both of which are entirely valid. For a while, I operated under the assumption that peak productivity required being a morning person. But as I learned about the schedules of people who I looked up to and compared my productivity and output to, I realized how subjective it really is. I think I heard that Demis Hassabis works best between 1 am and 4am, when the world is quiet and distractions are minimal, and as a result, he starts his day around noon. That perspective helped me reframe what “discipline” looks like, which is to try but optimizing for your own rhythms. That said, one non-negotiable I discovered: sleep. For me, anything less than seven hours means the rest of the day is running on borrowed time.</p> <hr/> <h2 id="2-ignore-noise-optimize-slowly">2. Ignore noise. Optimize slowly.</h2> <p>College, especially early on, can feel like a race. Everyone seems to have a plan. The pressure to pick a field, get into the right clubs, and land the most prestigious internships starts almost immediately. But from what I have noticed, the my peers who seemed the most confident by graduation weren’t the ones who sprinted toward something. They were the ones who eased in, took time to explore, and allowed themselves space to think critically about what actually interested them. My advice: treat the first year as a tasting menu. Spend time “sampling” different areas of study while keeping a few core skills sharp. Skills like writing, coding, public speaking, and people skills are portable and can be applied across many disciplines. Keeping those in good shape will ensure you never feel completely behind, even while you’re still figuring out what direction to take.</p> <hr/> <h2 id="3-mentor">3. Mentor.</h2> <p>Mentorship has become something of a buzzword, but its value is real. Having someone a few steps ahead of you, whether it is an upperclassman, a PhD student, or a professor, can make a real difference in how you navigate your time. They help you reason through decisions, avoid inefficient paths, and offer a clearer view of what lies ahead based on where they have already been. I tend to think of life as a kind of traversal problem. Everyone is moving through their own branching structure, and while no single path works for everyone, talking to those further along helps you avoid revisiting the same subtrees. Mentors offer insight into which directions led to meaningful experiences, which ones felt like dead ends, and which unexplored routes might be worth revisiting. The thing is that these relationships take effort. You have to give someone a reason to invest in you. In my experience, asking good questions, being open about your uncertainty, and approaching conversations with genuine curiosity helped build trust. Mentorship is not something you are simply given. It grows through effort and mutual interest. From a practical standpoint, it is also important to have two or three people who know you well and can vouch for you when it matters. Whether or not you are headed toward academia, having those references in your corner can quietly open more doors than you might expect.</p> <hr/> <h2 id="4-cohort">4. Cohort.</h2> <p>In tangent to that point, find a cohort. A group of people who share similar values and goals can help keep you accountable and push you to grow. It can also be beneficial to include people with different viewpoints, since a mix of perspectives often leads to deeper understanding. Going back to the tree metaphor, if you are all traversing toward similar goals at the same time, you can learn from each other’s mistakes and adjust your own approach accordingly. Collaborating in this way creates a system where everyone gets better. There are many ways to find this kind of group. Clubs, classes, research teams, or even online platforms can serve as starting points. It does not have to be limited to people at your school or in your grade. I found some sick people through Discord who shared my interests, and none of them went to UVA.</p> <hr/> <h2 id="learning-in-class">Learning in class.</h2> <p>Recorded lectures are great. If a class is being recorded and the lecture is large enough that you cannot ask questions during class, I usually find it more effective to just watch the recording instead. In many of these massive lectures, the lines to speak with the professor afterward are long, and the setting itself is not built for interaction. Watching the recording gives you full control. My method is to have the lecture on one screen and your LLM of choice on another. You can give the model the content or context of the class, pause the lecture whenever something is unclear, and use the model to explore your confusion. Language models have gotten good enough that the answers you get are often quite reliable and detailed. As a separate point, I highly recommend enrolling in smaller, more intimate classes when possible. These tend to be higher level courses, and the professors leading them are often more specialized and open to deeper questions. In my experience, these classes also tend to have fewer filler assignments and more opportunities for real engagement with the material.</p> <hr/> <h2 id="7-physical-exercise">7. Physical exercise.</h2> <p>Allot some time for exercise. There is a beauty in physical activity, and for me, it became a necessary break from the constant noise and stress that comes with college life. When I go to the gym, play a sport, or go for a run, the only thing on my mind is the activity itself. It creates a space where everything else fades into the background. There is something deeply satisfying about the adrenaline that comes from pushing your body and focusing completely on the moment. I honestly believe that without at least an hour of physical activity each day, I would have burned out completely. It gave me structure, clarity, and a way to reset when everything else felt chaotic.</p> <hr/>]]></content><author><name></name></author><category term="random thoughts"/><summary type="html"><![CDATA[Reflections after graduating from the University of Virginia]]></summary></entry></feed>