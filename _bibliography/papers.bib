---
---

@inproceedings{reddyAudioInsightDetectingSocial2024,
  abstract={
  During social interactions, understanding the intricacies of the context can be vital, particularly for socially anxious individuals. While previous research has found that the presence of a social interaction can be detected from ambient audio, the nuances within social contexts, which influence how anxiety provoking interactions are, remain largely unexplored. As an alternative to traditional, burdensome methods like self-report, this study presents a novel approach that harnesses ambient audio segments to detect social threat contexts. We focus on two key dimensions: number of interaction partners (dyadic vs. group) and degree of evaluative threat (explicitly evaluative vs. not explicitly evaluative). Building on data from a Zoom-based social interaction study (N=52 college students, of whom the majority N=45 are socially anxious), we employ deep learning methods to achieve strong detection performance. Under sample-wide 5-fold Cross Validation (CV), our model distinguished dyadic from group interactions with 90\% accuracy and detected evaluative threat at 83\%. Using a leave-one-group-out CV, accuracies were 82\% and 77\%, respectively. While our data are based on virtual interactions due to pandemic constraints, our method has the potential to extend to diverse real-world settings. This research underscores the potential of passive sensing and AI to differentiate intricate social contexts, and may ultimately advance the ability of context-aware digital interventions to offer personalized mental health support.
  },
  author={Varun Reddy, Zhiyuan Wang, Emma Toner, Max Larrazabal, Mehdi Boukhechba, Bethany A. Teachman, Laura E. Barnes},
  title={Rapport Matters: Enhancing HIV mHealth Communication through Linguistic Analysis and Large Language Models},
  booktitle={12th International Conference on Affective Computing and Intelligent Interaction (ACII) 2024.},
  year={2024},
  abbr={ACII},
  award={Oral},
  pdf={https://arxiv.org/abs/2407.14458},
}

@inproceedings{reddyCHI2024,
  abstract={
  In HIV care, a strong rapport between patient and provider is essential for strengthening trust, enhancing therapy adherence, and ultimately leading to improved health outcomes. As the adoption of digital interactions in HIV care via mobile health (mHealth) tools is emerging, maintaining rapport in these asynchronous text-based communications becomes a critical yet challenging task. In this paper, we analyze 1,740 messages from an mHealth platform, categorized by experienced clinicians as either ‘rapport-building’ or ‘information-only.’ We utilize linguistic analysis to uncover key attributes of rapport-building communication. This led to a set of machine learning (ML) models and Large Language Models (LLMs) capable of classifying these communication styles. Further, we propose the application of LLMs not only to identify but also to actively rewrite ‘information only’ messages into versions that enhance rapport building without compromising information integrity. Our research demonstrates potential advancements in HIV mHealth communication by integrating linguistic analysis with language models, leading to more effective patient-provider interactions.
  },
  author={Zhiyuan Wang, Varun Reddy, Karen Ingersoll, Tabor Flickinger, Laura E. Barnes},
  title={AudioInsight: Detecting Social Contexts Relevant to Social Anxiety from Speech},
  booktitle={ACM CHI conference on Human Factors in Computing Systems},
  year={2024},
  abbr={CHI},
  pdf={https://dl.acm.org/doi/10.1145/3613905.3651077},
}





